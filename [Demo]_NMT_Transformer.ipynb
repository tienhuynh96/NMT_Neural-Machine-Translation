{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tienhuynh96/NMT_Neural-Machine-Translation/blob/main/%5BDemo%5D_NMT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**EN-VI Machine Translation using Transformer Model**"
      ],
      "metadata": {
        "id": "ic0MvWoNA0HJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataset**"
      ],
      "metadata": {
        "id": "i-Wp-wD1A6eD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uS94DWsqlxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25258f3-e1a5-4e2b-d7a6-38d6c583d73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.1 Load dataset from dataset library**\n",
        "\n",
        "The IWSLT'15 English-Vietnamese data is used from Stanford NLP group."
      ],
      "metadata": {
        "id": "UK5uU8SPLdpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\n",
        "    \"mt_eng_vietnamese\",\n",
        "    \"iwslt2015-en-vi\"\n",
        ")"
      ],
      "metadata": {
        "id": "_A4DDN_Dr3PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfrGNPHgsZ0q",
        "outputId": "b772b0b6-8de5-4f18-e7d6-e1f48fb19919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['translation'],\n",
              "    num_rows: 133318\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']['translation'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3dNuG7EBCjY",
        "outputId": "22aa5ba9-868e-4d3c-9a3f-213149889c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'Rachel Pike : The science behind a climate headline',\n",
              " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_vJ3oMJQDmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.2 Incase can not load dataset from dataset**\n",
        "\n",
        "Wget data from github: https://github.com/stefan-it/nmt-en-vi"
      ],
      "metadata": {
        "id": "C604hT4JQGR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incase can not load dataset from dataset\n",
        "# Try\n",
        "# Wget data from github: https://github.com/stefan-it/nmt-en-vi\n",
        "# Train file\n",
        "!wget \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz\"\n",
        "# Dev file\n",
        "!wget \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/dev-2012-en-vi.tgz\"\n",
        "# Test file\n",
        "!wget \"https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\""
      ],
      "metadata": {
        "id": "zcB3PSkcQJj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9aa4db-9480-44cf-b314-c64e5423a149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-29 07:43:58--  https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/train-en-vi.tgz [following]\n",
            "--2024-05-29 07:43:58--  https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/train-en-vi.tgz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9903559 (9.4M) [application/octet-stream]\n",
            "Saving to: ‘train-en-vi.tgz’\n",
            "\n",
            "train-en-vi.tgz     100%[===================>]   9.44M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-05-29 07:43:59 (137 MB/s) - ‘train-en-vi.tgz’ saved [9903559/9903559]\n",
            "\n",
            "--2024-05-29 07:43:59--  https://github.com/stefan-it/nmt-en-vi/raw/master/data/dev-2012-en-vi.tgz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/dev-2012-en-vi.tgz [following]\n",
            "--2024-05-29 07:43:59--  https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/dev-2012-en-vi.tgz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103520 (101K) [application/octet-stream]\n",
            "Saving to: ‘dev-2012-en-vi.tgz’\n",
            "\n",
            "dev-2012-en-vi.tgz  100%[===================>] 101.09K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-29 07:44:00 (5.68 MB/s) - ‘dev-2012-en-vi.tgz’ saved [103520/103520]\n",
            "\n",
            "--2024-05-29 07:44:00--  https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/test-2013-en-vi.tgz [following]\n",
            "--2024-05-29 07:44:00--  https://raw.githubusercontent.com/stefan-it/nmt-en-vi/master/data/test-2013-en-vi.tgz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102940 (101K) [application/octet-stream]\n",
            "Saving to: ‘test-2013-en-vi.tgz’\n",
            "\n",
            "test-2013-en-vi.tgz 100%[===================>] 100.53K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-29 07:44:00 (6.24 MB/s) - ‘test-2013-en-vi.tgz’ saved [102940/102940]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the tgz file\n",
        "!tar -xzf train-en-vi.tgz\n",
        "!tar -xzf dev-2012-en-vi.tgz\n",
        "!tar -xzf test-2013-en-vi.tgz"
      ],
      "metadata": {
        "id": "rnFcD2a7QJgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset shape similar with datashape which download from datasets library\n",
        "data = {\n",
        "    \"train\": {\n",
        "                \"translation\":[]\n",
        "              },\n",
        "    \"validation\": {\n",
        "                \"translation\":[]\n",
        "              },\n",
        "    \"test\": {\n",
        "                \"translation\":[]\n",
        "              },\n",
        "}"
      ],
      "metadata": {
        "id": "Oa7s3SEhQJY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "E8kL6CtgQPBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e542123b-b253-4d27-9eca-3b944706f2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'translation': []},\n",
              " 'validation': {'translation': []},\n",
              " 'test': {'translation': []}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add path for train, test, validation file\n",
        "# Train file\n",
        "train_source_file = \"/content/train.en\"\n",
        "train_target_file = \"/content/train.vi\"\n",
        "# Validation file\n",
        "val_source_file = \"/content/tst2012.en\"\n",
        "val_target_file = \"/content/tst2012.vi\"\n",
        "# Test file\n",
        "test_source_file = \"/content/tst2013.en\"\n",
        "test_target_file = \"/content/tst2013.vi\""
      ],
      "metadata": {
        "id": "qp2LSJelQO8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build function: Get examples from sourse, target file and append into data\n",
        "def generate_examples(source_file, target_file, data, dataname):\n",
        "    # Open and read source file and target file of \"train, test, validation\" data\n",
        "    with open(source_file, encoding=\"utf-8\") as f:\n",
        "        source_sentences = f.read().split(\"\\n\")\n",
        "    with open(target_file, encoding=\"utf-8\") as f:\n",
        "        target_sentences = f.read().split(\"\\n\")\n",
        "\n",
        "    # Add examples of \"train, test, validation\" data\n",
        "    data_gen = data[dataname]['translation']\n",
        "    source, target = \"en\", \"vi\"\n",
        "    for idx, (l1, l2) in enumerate(zip(source_sentences, target_sentences)):\n",
        "        result = {source: l1, target: l2}\n",
        "        data_gen.append(result)\n",
        "    return data"
      ],
      "metadata": {
        "id": "Km2bhtr0QO5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_examples(source_file=train_source_file, target_file=train_target_file,\n",
        "                        data=data, dataname=\"train\")\n",
        "\n",
        "data = generate_examples(test_source_file, test_target_file, data, \"test\")\n",
        "data = generate_examples(val_source_file, val_target_file, data, \"validation\")"
      ],
      "metadata": {
        "id": "bmIr6UC1QO2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']['translation'][0]"
      ],
      "metadata": {
        "id": "USz8Y11SQXYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14915f19-86db-4035-8574-ea2dadb93167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'Rachel Pike : The science behind a climate headline',\n",
              " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['test']['translation'][0]"
      ],
      "metadata": {
        "id": "V0qE65a_QXMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb835c3e-6142-4ab6-cb20-62c4f587b8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;',\n",
              " 'vi': 'Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['validation']['translation'][0]"
      ],
      "metadata": {
        "id": "166nAOwxQZ_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa8a6fb-e109-4ba9-b150-766a9ff09f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?',\n",
              " 'vi': 'Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tokenization**"
      ],
      "metadata": {
        "id": "zezwqsDPBHbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get_tokenizer and build vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "I1hEFDKDt0yD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad999d3-5098-425f-df0e-924b2156213f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set source and target language\n",
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'vi'\n",
        "\n",
        "# Create token_transform and vocab_transform is empty dictionary\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "# Add tokenizer for source and target language\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('basic_english')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('basic_english')\n",
        "\n",
        "# Set special symbols and its id\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>','eos']"
      ],
      "metadata": {
        "id": "iLAjd8cit5rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab\n",
        "# Build yield tokens function: input data_iter and language (en or vi)\n",
        "def yield_tokens(data_iter, lang):\n",
        "  for data_sample in data_iter['translation']:\n",
        "    yield token_transform[lang](data_sample[lang])\n",
        "\n",
        "# Build vocab for source and target language\n",
        "for lang in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  # Get data from train data\n",
        "  train_iter = data['train']\n",
        "\n",
        "  # Create torchtext's vocab object\n",
        "  vocab_transform[lang] = build_vocab_from_iterator(\n",
        "      yield_tokens(train_iter, lang),   # yield for each language\n",
        "      min_freq=1,               # Minimum of appear tokens in vocab is stored\n",
        "      specials=special_symbols, # Get special symbols\n",
        "      special_first=True        # Set special symbol ids is first\n",
        "  )\n",
        "\n",
        "  # Set set_default_index is UNK_IDX\n",
        "  vocab_transform[lang].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "ceJcN3CKyMEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab for source language\n",
        "vocab_transform[SRC_LANGUAGE].get_itos()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBFvFZxt0om-",
        "outputId": "0cd23b07-3c5e-4b30-9565-56d635714d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<bos>', 'eos', ',', '.', 'the', 'and', 'to', '&apos']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab for target language\n",
        "vocab_transform[TGT_LANGUAGE].get_itos()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytn1Z9vT0s_V",
        "outputId": "ff3ec359-6fed-4b12-d4b9-023b5e371aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<bos>', 'eos', ',', '.', 'và', 'tôi', 'là', 'một']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_transform[SRC_LANGUAGE]), len(vocab_transform[TGT_LANGUAGE])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLJaXdTe00v-",
        "outputId": "4ad3c09b-c2b0-4d9d-a5f3-ff306e22ca96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47270, 21113)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataloader**"
      ],
      "metadata": {
        "id": "aiKbMbtVBLCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence     # padding follow the highest sequence length in dataset\n",
        "\n",
        "# Set max len\n",
        "MAX_LEN = 100\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "# Take a series of transformation functions as arguments and returns a new function\n",
        "def sequential_transforms(*transforms):   # *transforms allows the function to accept any number of transformation functions as arguments.\n",
        "    def func(txt_input): # inner function\n",
        "        # Applying Transformations in Sequence\n",
        "        # For each transformation, it applies the function to txt_input, updating txt_input with the transformed result.\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices: BOS-ids-EOS\n",
        "def tensor_transform(token_ids):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# Function to truncate\n",
        "def truncate(sample):\n",
        "    if sample.size(0) > MAX_LEN:\n",
        "        return sample[:MAX_LEN, :]\n",
        "    else:\n",
        "        return sample"
      ],
      "metadata": {
        "id": "vTX40fEM3OhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for lang in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[lang] = sequential_transforms(\n",
        "        token_transform[lang], # Tokenization   => tokens\n",
        "        vocab_transform[lang], # Numericalization => ids\n",
        "        tensor_transform # Add BOS/EOS and create tensor  => bos ids eos\n",
        "    )\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    # transforms for each sample in batch\n",
        "    for sample in batch:\n",
        "        src_sample, tgt_sample = sample[SRC_LANGUAGE], sample[TGT_LANGUAGE]\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample).to(dtype=torch.int64))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample).to(dtype=torch.int64))\n",
        "\n",
        "    # Add pad sequence and truncate\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    src_batch = truncate(src_batch)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = truncate(tgt_batch)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "4fdIO0Q3RLls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Build dataloader for train, validation, test data\n",
        "train_dataloader = DataLoader(\n",
        "    data['train']['translation'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    data['validation']['translation'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    data['test']['translation'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "Xj48YRZs8-NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data of first batch\n",
        "src_ids, tgt_ids = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "_YILfBe-RT_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check shape\n",
        "src_ids.shape, tgt_ids.shape"
      ],
      "metadata": {
        "id": "pWSRu7smRT2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73df636-6f46-4708-bc1b-c34dd7fca52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 52]), torch.Size([32, 78]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_ids[0]"
      ],
      "metadata": {
        "id": "rVldiDQARSoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0e3ec4-94a3-43d6-fcb0-14809a092b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    2,  6429, 17576,     6,   295,   553,    11,   682,  5334,     3,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the source ids\n",
        "' '.join([vocab_transform[SRC_LANGUAGE].lookup_token(token) for token in src_ids[0]])"
      ],
      "metadata": {
        "id": "OO-7qJS8RSMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3fa09cb3-dd66-485d-a406-de801fdea5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos> rachel pike the science behind a climate headline eos <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the target ids\n",
        "' '.join([vocab_transform[TGT_LANGUAGE].lookup_token(token) for token in tgt_ids[0]])"
      ],
      "metadata": {
        "id": "YsE9tunvRR-q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3fa93610-90b7-4c8a-aba5-4ce2af32d946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos> khoa học đằng sau một tiêu đề về khí hậu eos <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model**"
      ],
      "metadata": {
        "id": "Tv79ODxTBOcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    # Add positional encoding to token embedding\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "by_JLG9NdCHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        # Transofrmer is sequence to sequence model\n",
        "        self.transformer = Transformer(d_model=emb_size,    # Embedding size\n",
        "                                       nhead=nhead,         # n head\n",
        "                                       num_encoder_layers=num_encoder_layers,   # Number encoder layers\n",
        "                                       num_decoder_layers=num_decoder_layers,   # Number decoder layers\n",
        "                                       dim_feedforward=dim_feedforward,         # Feedforward dimention\n",
        "                                       dropout=dropout,                         # Dropout\n",
        "                                       batch_first=True)\n",
        "        # generator is linear function\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        # Token embedding for source token\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        # Token embedding for target token\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        # Add position for encode and decode\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,       # For seperate the token and padding\n",
        "                tgt_mask: Tensor,       # For seperate the token and padding\n",
        "                src_padding_mask: Tensor,   # Mask the token each time step to train\n",
        "                tgt_padding_mask: Tensor,   # Mask the token each time step to train\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        # Compute src_emb: 1- compute src_tok_emb (source token embedding), 2 - compute positional_encoding\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        # Compute tgt_emb: 1- compute tgt_tok_emb (target token embedding), 2 - compute positional_encoding\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        # Compute transformer output\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)      # B x S X H\n",
        "        # Generator in vocab size\n",
        "        return self.generator(outs)         # B X S X target vocab size\n",
        "\n",
        "    # Create encoder\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    # Create decoder\n",
        "    # Memory is output state of encoder\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "Vj6MGFIyNYvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(torch.triu(torch.ones((6, 6), device=DEVICE))  == 1).transpose(0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf8vVZ3OT3O6",
        "outputId": "dac8ddb6-a77b-4095-d874-998c853907fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True, False, False, False, False, False],\n",
              "        [ True,  True, False, False, False, False],\n",
              "        [ True,  True,  True, False, False, False],\n",
              "        [ True,  True,  True,  True, False, False],\n",
              "        [ True,  True,  True,  True,  True, False],\n",
              "        [ True,  True,  True,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros((6, 6),device=DEVICE).type(torch.bool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_2fL2DnWaP3",
        "outputId": "f7b05840-4571-410e-f28d-4094fbaa84f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False],\n",
              "        [False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build function to create mask matrix,\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    # Create one matrix => triu matrix => compare with 1 => transpose to have type and shape of mask token\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    # Fill: 0 value -> -inf; 1 value -> 0:\n",
        "    # if value is -inf, when softmax we get the value 0\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "# Build function to create mask for target and source input\n",
        "def create_mask(src, tgt):\n",
        "    # Get len of sequence in source\n",
        "    src_seq_len = src.shape[1]\n",
        "    # Get len of sequence in target\n",
        "    tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "    # Create target mask\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    # Create source mask: a zero matrix or source do not need mask\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    # Create source padding mask to figure out where is token and padding\n",
        "    src_padding_mask = (src == PAD_IDX)\n",
        "    # Create target padding mask to figure out where is token and padding\n",
        "    tgt_padding_mask = (tgt == PAD_IDX)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "8oay0NebjbzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add parameter for model\n",
        "# Add parameter values\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "# Create transformer model and put to device\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "# Create loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Create otimizer\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "v0cGHmm0jrHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "# Get values of the first batch and put to device\n",
        "src_ids, tgt_ids = next(iter(train_dataloader))\n",
        "src_ids = src_ids.to(DEVICE)\n",
        "tgt_ids = tgt_ids.to(DEVICE)\n",
        "# Get target input\n",
        "tgt_input = tgt_ids[:, :-1]\n",
        "# Get target output\n",
        "tgt_output = tgt_ids[:, 1:]\n",
        "# create mask\n",
        "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ids, tgt_input)\n",
        "# Compute model: logits # B X S X target vocab size\n",
        "logits = transformer(\n",
        "    src_ids, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask\n",
        ")\n",
        "# Compute loss\n",
        "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))"
      ],
      "metadata": {
        "id": "-XdkcN2PjlxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d476ca-f4ce-4797-ae63-7c733b81cf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJA5_bMMmTfU",
        "outputId": "49805bbc-fc31-4d8d-ff6c-c0a5e9331134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 77, 21113])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcsMbavWmVEC",
        "outputId": "1c629b47-b1fb-450c-8ca0-65ebf0999252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.0856, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Trainer**"
      ],
      "metadata": {
        "id": "hzudM9B3BQyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Creaete train epoch function\n",
        "def train_epoch(model, optimizer, criterion, train_dataloader, device):\n",
        "    #  train mode\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    # Iterator to get data from train_loader\n",
        "    for src_ids, tgt_ids in train_dataloader:\n",
        "        # put src_ids, tgt_ids to device\n",
        "        src_ids = src_ids.to(device)\n",
        "        tgt_ids = tgt_ids.to(device)\n",
        "\n",
        "        # Get input and output target\n",
        "        tgt_input = tgt_ids[:, :-1]\n",
        "        tgt_output = tgt_ids[:, 1:]\n",
        "\n",
        "        # Create mask\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ids, tgt_input)\n",
        "        # Compute model: # B X S X target vocab size\n",
        "        output = model(\n",
        "                src_ids, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        # Set optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(\n",
        "            output.reshape(-1, output.shape[-1]),\n",
        "            tgt_output.reshape(-1))\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weight\n",
        "        optimizer.step()\n",
        "        # save loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "# Compute loss\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    # eval mode\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for src_ids, tgt_ids in data_loader:\n",
        "            # put src_ids, tgt_ids to device\n",
        "            src_ids = src_ids.to(device)\n",
        "            tgt_ids = tgt_ids.to(device)\n",
        "\n",
        "            # Get input and output target\n",
        "            tgt_input = tgt_ids[:, :-1]\n",
        "            tgt_output = tgt_ids[:, 1:]\n",
        "\n",
        "            # Create mask\n",
        "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ids, tgt_input)\n",
        "            # Compute output\n",
        "            output = model(\n",
        "                src_ids, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask\n",
        "            )\n",
        "            # Compute loss\n",
        "            loss = criterion(\n",
        "                output.reshape(-1, output.shape[-1]),\n",
        "                tgt_output.reshape(-1)\n",
        "            )\n",
        "            # Save loss\n",
        "            losses.append(loss.item())\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "# Creaete train function\n",
        "def train(model, train_dataloader, valid_dataloader, optimizer, criterion, device, epochs):\n",
        "    # Iterator for each epoch\n",
        "    for epoch in tqdm(range(1, epochs+1)):\n",
        "        # Set start time\n",
        "        start_time = time.time()\n",
        "        # Compute train loss\n",
        "        train_loss = train_epoch(model, optimizer, criterion, train_dataloader, device)\n",
        "        # Compute validation loss\n",
        "        valid_loss = evaluate(model, valid_dataloader, criterion, device)\n",
        "        # Set end time\n",
        "        end_time = time.time()\n",
        "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {valid_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "metadata": {
        "id": "Q0NxKwpIBS7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training**"
      ],
      "metadata": {
        "id": "2duk3dBAyVzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create parameter input\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "# Create model and put to device\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "# Set criterion\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "# Set optimizer\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "# Set epoch\n",
        "epochs = 1\n",
        "# train model\n",
        "train(transformer, train_dataloader, valid_dataloader, optimizer, criterion, DEVICE, epochs)"
      ],
      "metadata": {
        "id": "qw66lVTda2Dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081aaa18-7bd0-4634-fc1f-382cbad2a727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Inference**"
      ],
      "metadata": {
        "id": "6Eik34pouuwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build greedy decode function\n",
        "# generate next token base on previous predicted tokens\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    # Put source to device\n",
        "    src = src.to(DEVICE)\n",
        "    # Put source mask to device\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    # Compute out put of encoder (get hidden state)\n",
        "    memory = model.encode(src, src_mask)\n",
        "    # Create start tokens as start symbol\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\n",
        "    # Iterator to generate next token untill meet EOS_IDS or max_len\n",
        "    for i in range(max_len-1):\n",
        "        # Put memory to device\n",
        "        memory = memory.to(DEVICE)\n",
        "        # Create target mask\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        # Compute out put of decoder\n",
        "        out = model.decode(ys, memory, tgt_mask)  # B, S, H\n",
        "        # transpose, to get squence (B, S) => (S, B)\n",
        "        out = out.transpose(0, 1)\n",
        "        # Compute generator\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # Get the index of word\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        # Get next word\n",
        "        next_word = next_word[-1].item() # index\n",
        "\n",
        "        # Update start input token index\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        # If the next word is EOS IDS, break\n",
        "        if next_word == EOS_IDX:  #ESO : 3\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    # test transform for source sentence\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(1, -1)\n",
        "    # number of tokens in source sentence\n",
        "    num_tokens = src.shape[1]\n",
        "    # Create mask\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    # Generate next token\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(\n",
        "        vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "bhDogfSFuufE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test function\n",
        "translate(transformer, \"i go to school\")"
      ],
      "metadata": {
        "id": "x6QpFuZNu75i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metric: sacrebleu\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "pred_sentences, tgt_sentences = [], []\n",
        "for sample in tqdm(data['test']['translation']):\n",
        "    # Get data from test\n",
        "    src_sentence = sample[SRC_LANGUAGE]\n",
        "    tgt_sentence = sample[TGT_LANGUAGE]\n",
        "\n",
        "    # translate\n",
        "    pred_sentence = translate(transformer, src_sentence)\n",
        "    pred_sentences.append(pred_sentence)\n",
        "\n",
        "    tgt_sentences.append(tgt_sentence)\n",
        "# Compute metric\n",
        "bleu_score = sacrebleu.corpus_bleu(pred_sentences, [tgt_sentences], force=True)\n",
        "bleu_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV0LshoYzc68",
        "outputId": "b2e9cbcd-328f-4323-f932-72d5bac09752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1269/1269 [01:53<00:00, 11.14it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 6.91 46.8/16.8/6.4/2.4 (BP = 0.659 ratio = 0.706 hyp_len = 23819 ref_len = 33738)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    }
  ]
}